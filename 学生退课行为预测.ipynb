{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiteraturePro/MOOCCube/blob/master/%E5%AD%A6%E7%94%9F%E9%80%80%E8%AF%BE%E8%A1%8C%E4%B8%BA%E9%A2%84%E6%B5%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载数据集\n",
        "import os\n",
        "%cd /content\n",
        "# dowload the file\n",
        "if not os.path.exists('MCubeData.zip'):\n",
        "  !gdown --id 1kBWqOG3SZYyeTHCz249sJlVILl0HBnpn \\\n",
        "          -O MCubeData.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C50uYYtnyIRR",
        "outputId": "e12c2582-576b-401a-9901-b1a76218209d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kBWqOG3SZYyeTHCz249sJlVILl0HBnpn\n",
            "To: /content/MCubeData.zip\n",
            "100% 594M/594M [00:07<00:00, 81.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 解压数据集文件\n",
        "!unzip -o MCubeData.zip -d /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzpaGLagyIHa",
        "outputId": "3bc7ae1c-2b9e-4f11-e436-0740008233f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  MCubeData.zip\n",
            "   creating: /content/MCubeData/\n",
            "  inflating: /content/MCubeData/course_info.json  \n",
            "  inflating: /content/MCubeData/problem_act_test_new.json  \n",
            "  inflating: /content/MCubeData/task1_sample.json  \n",
            "  inflating: /content/MCubeData/user_video_act_train_1.json  \n",
            "  inflating: /content/MCubeData/user_video_act_train_2.json  \n",
            "  inflating: /content/MCubeData/user_video_act_train_3.json  \n",
            "  inflating: /content/MCubeData/user_video_act_train_4.json  \n",
            "  inflating: /content/MCubeData/user_video_act_val_triple_withId_noLabel_1.json  \n",
            "  inflating: /content/MCubeData/user_video_act_val_triple_withId_noLabel_2.json  \n",
            "  inflating: /content/MCubeData/video_info.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tree -y\n",
        "!cd /content/MCubeData\n",
        "!tree\n",
        "!cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzIZ_YiVyHez",
        "outputId": "ca4d86b7-ab23-4344-b4e4-eb5e071d8454"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (79.6 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            ".\n",
            "├── MCubeData\n",
            "│   ├── course_info.json\n",
            "│   ├── problem_act_test_new.json\n",
            "│   ├── task1_sample.json\n",
            "│   ├── user_video_act_train_1.json\n",
            "│   ├── user_video_act_train_2.json\n",
            "│   ├── user_video_act_train_3.json\n",
            "│   ├── user_video_act_train_4.json\n",
            "│   ├── user_video_act_val_triple_withId_noLabel_1.json\n",
            "│   ├── user_video_act_val_triple_withId_noLabel_2.json\n",
            "│   └── video_info.json\n",
            "├── MCubeData.zip\n",
            "└── sample_data\n",
            "    ├── anscombe.json\n",
            "    ├── california_housing_test.csv\n",
            "    ├── california_housing_train.csv\n",
            "    ├── mnist_test.csv\n",
            "    ├── mnist_train_small.csv\n",
            "    └── README.md\n",
            "\n",
            "2 directories, 17 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:54:59.258839Z",
          "start_time": "2020-11-01T00:54:57.307869Z"
        },
        "id": "eXva7Emdx9hP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "# import torch\n",
        "# from torch import nn,optim\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "from itertools import chain\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import warnings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "import json\n",
        "# from pandarallel import pandarallel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialization\n",
        "# pandarallel.initialize(progress_bar=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:00.293001Z",
          "start_time": "2020-11-01T00:54:59.258839Z"
        },
        "id": "xTXwW_2cx9hT"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "from io import StringIO\n",
        "\n",
        "def read_json(path):\n",
        "    file = open(path, \"r\", encoding=\"utf-8\")\n",
        "    text = file.read()\n",
        "    text = text.replace('\\'', '\\\"')\n",
        "    text = '[' + text.replace('}{','},{') + ']'\n",
        "    data = pd.read_json(StringIO(text))\n",
        "    return data\n",
        "noLabel = read_json('./MCubeData/user_video_act_val_triple_withId_noLabel_2.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:00.311992Z",
          "start_time": "2020-11-01T00:55:00.295009Z"
        },
        "id": "XLeB0wVYx9hU"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "from io import StringIO\n",
        "\n",
        "def read_json(path):\n",
        "    file = open(path, \"r\", encoding=\"utf-8\")\n",
        "    text = file.read()\n",
        "    text = '[' + text.replace('\\n',',')[:-1] + ']'\n",
        "    data = pd.read_json(StringIO(text))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:18.528176Z",
          "start_time": "2020-11-01T00:55:00.313764Z"
        },
        "id": "uxlHFIfBx9hV"
      },
      "outputs": [],
      "source": [
        "course_info = read_json('./MCubeData/course_info.json')\n",
        "video_info = read_json('./MCubeData/video_info.json')\n",
        "train = pd.concat([read_json('./MCubeData/user_video_act_train_1.json'), read_json('./MCubeData/user_video_act_train_2.json')], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:18.589937Z",
          "start_time": "2020-11-01T00:55:18.529241Z"
        },
        "id": "IrySENMAx9hV"
      },
      "outputs": [],
      "source": [
        "course_encode = LabelEncoder().fit(course_info['course_id'].tolist())\n",
        "video_encode = LabelEncoder().fit(video_info['id'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:18.605894Z",
          "start_time": "2020-11-01T00:55:18.590934Z"
        },
        "id": "bvWrD2Srx9hW"
      },
      "outputs": [],
      "source": [
        "train['item_id'] = train.index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsMqZICkx9hX"
      },
      "source": [
        "# create features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:55:18.621852Z",
          "start_time": "2020-11-01T00:55:18.607892Z"
        },
        "code_folding": [],
        "id": "H42aiKz0x9hZ"
      },
      "outputs": [],
      "source": [
        "def transform_data(item, istrain=True):\n",
        "    result = pd.DataFrame.from_records(item['activity'])\n",
        "    result['item_id'] = item['item_id']\n",
        "    result.sort_values('local_start_time', inplace=True)\n",
        "    if istrain:\n",
        "        result['drop'] = result.course_id.map(dict(zip(item['course_list'], item['label_list'])))\n",
        "        result['drop'] = result.groupby('course_id')['drop'].transform(lambda x: [0]*(len(x)-1) + [x.iloc[-1]])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T00:59:10.190973Z",
          "start_time": "2020-11-01T00:55:18.624844Z"
        },
        "id": "kc8W2IQ2x9ha"
      },
      "outputs": [],
      "source": [
        "data = train.apply(transform_data, axis=1)\n",
        "data = pd.concat(data.to_list(), ignore_index=True)\n",
        "test = noLabel.apply(lambda x: transform_data(x, False), axis=1)\n",
        "test = pd.concat(test.to_list(), ignore_index=True)\n",
        "data = pd.concat([data, test], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBql4FzGx9ha"
      },
      "source": [
        "## couse list doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T01:00:12.617457Z",
          "start_time": "2020-11-01T00:59:10.190973Z"
        },
        "code_folding": [],
        "id": "r3rgTefLx9hb"
      },
      "outputs": [],
      "source": [
        "# doc2vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "course_documents = [\n",
        "    TaggedDocument(doc, [i]) for doc, i in zip(\n",
        "        data.groupby('item_id')['course_id'].apply(lambda x: x.tolist()),\n",
        "        data.groupby(data['item_id'])['item_id'].apply(lambda x: x.iloc[0]))\n",
        "]\n",
        "course_model = Doc2Vec(course_documents,\n",
        "                       vector_size=16,\n",
        "                       epochs=7,\n",
        "                       window=3,\n",
        "                       min_count=1,\n",
        "                       workers=16,\n",
        "                       seed=42,\n",
        "                       dm=0,\n",
        "                       hs=1,\n",
        "                       dbow_words=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgvn4PK2x9hc"
      },
      "source": [
        "## video list doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T01:02:04.046279Z",
          "start_time": "2020-11-01T01:00:12.617457Z"
        },
        "id": "mJsTwhwjx9hd"
      },
      "outputs": [],
      "source": [
        "video_documents = [\n",
        "    TaggedDocument(doc, [i]) for doc, i in zip(\n",
        "        data.groupby('item_id')['video_id'].apply(lambda x: x.tolist()),\n",
        "        data.groupby(data['item_id'])['item_id'].apply(lambda x: x.iloc[0]))\n",
        "]\n",
        "video_model = Doc2Vec(video_documents,\n",
        "                      vector_size=32,\n",
        "                      epochs=10,\n",
        "                      window=3,\n",
        "                      min_count=1,\n",
        "                      workers=16,\n",
        "                      seed=42,\n",
        "                      dm=0,\n",
        "                      hs=1,\n",
        "                      dbow_words=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PUS9Bhux9hd"
      },
      "source": [
        "## create_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T01:02:04.076000Z",
          "start_time": "2020-11-01T01:02:04.046279Z"
        },
        "code_folding": [
          4
        ],
        "id": "aGR6JotTx9he"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "def kfold_mean(df_train, df_test, target, target_mean_list):\n",
        "    folds = GroupKFold(n_splits=5)\n",
        "\n",
        "    mean_of_target = df_train[target].mean()\n",
        "\n",
        "    for fold_, (trn_idx, val_idx) in tqdm(\n",
        "            enumerate(folds.split(df_train, groups=df_train['item_id']))):\n",
        "        tr_x = df_train.iloc[trn_idx, :]\n",
        "        vl_x = df_train.iloc[val_idx, :]\n",
        "\n",
        "        for col in target_mean_list:\n",
        "            df_train.loc[vl_x.index, f'{col}_target_enc'] = vl_x[col].map(\n",
        "                tr_x.groupby(col)[target].mean())\n",
        "\n",
        "    for col in target_mean_list:\n",
        "        df_train[f'{col}_target_enc'].fillna(mean_of_target, inplace=True)\n",
        "\n",
        "        df_test[f'{col}_target_enc'] = df_test[col].map(\n",
        "            df_train.groupby(col)[f'{col}_target_enc'].mean())\n",
        "\n",
        "        df_test[f'{col}_target_enc'].fillna(mean_of_target, inplace=True)\n",
        "    return pd.concat([df_train, df_test], ignore_index=True)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def attn(u, v):\n",
        "    return np.sum(softmax(\n",
        "        1 - (u @ v.T /\n",
        "             (np.linalg.norm(u, 2) * np.linalg.norm(v, 2, axis=1)))).reshape(\n",
        "                 (-1, 1)) * v,\n",
        "                  axis=0).tolist()\n",
        "\n",
        "\n",
        "def processParallelAttn(df):\n",
        "    return df.apply(\n",
        "        lambda y: attn(np.array(y), np.array(df.tolist()))).tolist()\n",
        "\n",
        "def applyParallel(dfGrouped, func):\n",
        "    result = []\n",
        "    tmp = Parallel(n_jobs=-1)(delayed(func)(group) for name, group in tqdm(dfGrouped))\n",
        "    for a in tmp:\n",
        "        result.extend(a)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T01:02:04.105920Z",
          "start_time": "2020-11-01T01:02:04.078993Z"
        },
        "code_folding": [],
        "id": "w_p3sF9Zx9he"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "\n",
        "def create_vec_features(data):\n",
        "\n",
        "    data[[f'course_vec{i+1}' for i in range(course_model.vector_size)\n",
        "          ]] = pd.DataFrame(data['course_id'].apply(\n",
        "              lambda x: course_model.wv.get_vector(x).tolist()).tolist(),\n",
        "                            index=data.index)\n",
        "    data[[f'video_vec{i+1}' for i in range(video_model.vector_size)\n",
        "          ]] = pd.DataFrame(data['video_id'].apply(\n",
        "              lambda x: video_model.wv.get_vector(x).tolist()).tolist(),\n",
        "                            index=data.index)\n",
        "\n",
        "    #     向量表示\n",
        "    data[[f'item_by_course_vec{i+1}' for i in range(course_model.vector_size)\n",
        "          ]] = pd.DataFrame(data['item_id'].apply(\n",
        "              lambda x: course_model.docvecs[x].tolist()).tolist(),\n",
        "                            index=data.index)\n",
        "    data[[f'item_by_video_vec{i+1}' for i in range(video_model.vector_size)\n",
        "          ]] = pd.DataFrame(data['item_id'].apply(\n",
        "              lambda x: video_model.docvecs[x].tolist()).tolist(),\n",
        "                            index=data.index)\n",
        "\n",
        "    data['cosine_of_item_course'] = data.apply(\n",
        "        lambda x: cosine(course_model.docvecs[x['item_id']],\n",
        "                         course_model.wv.get_vector(x['course_id'])),\n",
        "        axis=1)\n",
        "\n",
        "    print('course attn vec')\n",
        "    data['course_vec'] = data[[\n",
        "        f'course_vec{i+1}' for i in range(course_model.vector_size)\n",
        "    ]].values.tolist()\n",
        "    data[[\n",
        "        f'course_vec_attn_mean{i+1}' for i in range(course_model.vector_size)\n",
        "    ]] = pd.DataFrame(applyParallel(\n",
        "        data.groupby('item_id')['course_vec'], processParallelAttn),\n",
        "                      index=data.index)\n",
        "    del data['course_vec']\n",
        "\n",
        "    print('video attn vec')\n",
        "    data['video_vec'] = data[[\n",
        "        f'video_vec{i+1}' for i in range(video_model.vector_size)\n",
        "    ]].values.tolist()\n",
        "    data[[f'video_vec_attn_mean{i+1}' for i in range(video_model.vector_size)\n",
        "          ]] = pd.DataFrame(applyParallel(\n",
        "              data.groupby('item_id')['video_vec'], processParallelAttn),\n",
        "                            index=data.index)\n",
        "    del data['video_vec']\n",
        "\n",
        "    for i in tqdm(range(video_model.vector_size)):\n",
        "        data[f'video_vec_mean{i+1}'] = data.groupby(\n",
        "            ['item_id', 'course_id'])[f'video_vec{i+1}'].transform('mean')\n",
        "        data[f'video_vec_max{i+1}'] = data.groupby(\n",
        "            ['item_id', 'course_id'])[f'video_vec{i+1}'].transform('max')\n",
        "        data[f'video_vec_min{i+1}'] = data.groupby(\n",
        "            ['item_id', 'course_id'])[f'video_vec{i+1}'].transform('min')\n",
        "        data[f'video_vec_std{i+1}'] = data.groupby(\n",
        "            ['item_id', 'course_id'])[f'video_vec{i+1}'].transform('std')\n",
        "        data[f'video_vec_skew{i+1}'] = data.groupby(\n",
        "            ['item_id',\n",
        "             'course_id'])[f'video_vec{i+1}'].transform(lambda x: x.skew())\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T01:02:04.153792Z",
          "start_time": "2020-11-01T01:02:04.106918Z"
        },
        "code_folding": [
          0
        ],
        "id": "cxSXcc-Vx9hf"
      },
      "outputs": [],
      "source": [
        "def create_features(data):\n",
        "    data.sort_values(['item_id', 'local_start_time'], inplace=True)\n",
        "\n",
        "    #     course/video encode\n",
        "    print('course/video encode')\n",
        "\n",
        "    course_dict = course_info.set_index('course_id').to_dict(orient='index')\n",
        "\n",
        "    data['course_id_count'] = data.groupby(\n",
        "        data['course_id'])['course_id'].transform('count')\n",
        "    data['course_id_nunique_video'] = data.groupby(\n",
        "        data['course_id'])['video_id'].transform('nunique')\n",
        "    data['course_look_len_mean'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['video_id'].transform('count').groupby(\n",
        "            data['course_id']).transform('mean')\n",
        "    data['course_id_local_watching_time_mean'] = data.groupby(\n",
        "        data['course_id'])['local_watching_time'].transform('mean')\n",
        "\n",
        "    data['video_id_count'] = data.groupby(\n",
        "        data['video_id'])['video_id'].transform('count')\n",
        "    data['video_id_local_watching_time_mean'] = data.groupby(\n",
        "        data['video_id'])['local_watching_time'].transform('mean')\n",
        "\n",
        "    data['course_video'] = data['course_id'].astype(\n",
        "        str) + data['video_id'].astype(str)\n",
        "    data = kfold_mean(data[~data['drop'].isna()], data[data['drop'].isna()],\n",
        "                      'drop', ['course_id', 'video_id', 'course_video'])\n",
        "    del data['course_video']\n",
        "\n",
        "    data['video_loc'] = data.apply(\n",
        "        lambda x: course_dict[x['course_id']]['item'].index(x['video_id']),\n",
        "        axis=1)\n",
        "    data['course_len'] = data.apply(\n",
        "        lambda x: len(course_dict[x['course_id']]['item']), axis=1)\n",
        "    data['video_loc/course_len'] = data['video_loc'] / data['course_len']\n",
        "\n",
        "    #     time\n",
        "    print('time')\n",
        "\n",
        "    data['local_start_time'] = pd.to_datetime(data['local_start_time'])\n",
        "    data['local_end_time'] = pd.to_datetime(data['local_end_time'])\n",
        "\n",
        "    data['year'] = data['local_end_time'].dt.year\n",
        "    data['month'] = data['local_end_time'].dt.month\n",
        "    data['day'] = data['local_end_time'].dt.day\n",
        "    data['dayofweek'] = data['local_end_time'].dt.dayofweek\n",
        "    data['hour'] = data['local_end_time'].dt.hour\n",
        "\n",
        "    data = kfold_mean(data[~data['drop'].isna()], data[data['drop'].isna()],\n",
        "                      'drop', ['year', 'month', 'dayofweek'])\n",
        "\n",
        "    data['local_watching_time/video_duration'] = data[\n",
        "        'local_watching_time'] / data['video_duration']\n",
        "    data['local_watching_time/video_progress_time'] = data[\n",
        "        'local_watching_time'] / data['video_progress_time']\n",
        "    data['video_start_time/video_duration'] = data['video_start_time'] / data[\n",
        "        'video_duration']\n",
        "    data['video_end_time/video_duration'] = data['video_end_time'] / data[\n",
        "        'video_duration']\n",
        "    data['local_watching_time/end-start'] = data['local_watching_time'] / (\n",
        "        data['video_end_time'] - data['video_start_time'])\n",
        "\n",
        "    data['watch_time_interval'] = ((data['local_start_time'] - data.groupby(\n",
        "        'item_id')['local_end_time'].transform(lambda x: x.shift(1))) /\n",
        "                                   pd.Timedelta(hours=1)).fillna(0)\n",
        "\n",
        "    #     item\n",
        "    print('item')\n",
        "\n",
        "    #     整体统计\n",
        "    data['item_course_nunique'] = data.groupby(\n",
        "        'item_id')['course_id'].transform('nunique')\n",
        "\n",
        "    data['item_video_nunique'] = data.groupby('item_id')['video_id'].transform(\n",
        "        'nunique')\n",
        "    data['item_coures_video_nunique'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['video_id'].transform('nunique')\n",
        "\n",
        "    data['item_coures_video_watched_ratio'] = data[\n",
        "        'item_coures_video_nunique'] / data['course_len']\n",
        "\n",
        "    data['item_watching_count'] = data.groupby(\n",
        "        'item_id')['watching_count'].transform('sum')\n",
        "    data['item_coures_video_watching_count'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['watching_count'].transform('sum')\n",
        "    data['item_coures_video_watching_count/item_watching_count'] = data[\n",
        "        'item_coures_video_watching_count'] / data['item_watching_count']\n",
        "\n",
        "    data['item_watching_time'] = data.groupby(\n",
        "        'item_id')['local_watching_time'].transform('sum')\n",
        "    data['item_coures_video_watching_time'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['local_watching_time'].transform('sum')\n",
        "    data['item_coures_video_watching_time/item_watching_time'] = data[\n",
        "        'item_coures_video_watching_time'] / data['item_watching_time']\n",
        "\n",
        "    data['item_watching_time_interval_mean'] = data.groupby(\n",
        "        'item_id')['watch_time_interval'].transform('mean')\n",
        "    data['item_coures_video_watching_time_interval_mean'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['watch_time_interval'].transform('mean')\n",
        "\n",
        "    data['item_watching_time_interval_max'] = data.groupby(\n",
        "        'item_id')['watch_time_interval'].transform('max')\n",
        "    data['item_coures_video_watching_time_interval_max'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['watch_time_interval'].transform('max')\n",
        "\n",
        "    data['item_course_keep_time'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['local_start_time'].transform(\n",
        "            lambda x: (x.max() - x.min()) / pd.Timedelta(hours=1))\n",
        "\n",
        "    data['item_course_final_watch_time_interval'] = (data.groupby(\n",
        "        ['item_id'])['local_start_time'].transform('max') - data.groupby([\n",
        "            'item_id', 'course_id'\n",
        "        ])['local_start_time'].transform('max')) / pd.Timedelta(hours=1)\n",
        "\n",
        "    data['item_course_next_watch_time_interval'] = data.groupby(\n",
        "        'item_id')['watch_time_interval'].transform(\n",
        "            lambda x: x.shift(-1)).fillna(0)\n",
        "\n",
        "\n",
        "    #     item + course\n",
        "    print('item + course')\n",
        "    data['cosine_of_item_course'] = data.apply(\n",
        "        lambda x: cosine(course_model.docvecs[x['item_id']],\n",
        "                         course_model.wv.get_vector(x['course_id'])),\n",
        "        axis=1)\n",
        "\n",
        "    data['video_loc_mean'] = data.groupby(['item_id', 'course_id'\n",
        "                                           ])['video_loc'].transform('mean')\n",
        "    data['video_loc_min'] = data.groupby(['item_id', 'course_id'\n",
        "                                          ])['video_loc'].transform('min')\n",
        "    data['video_loc_max'] = data.groupby(['item_id', 'course_id'\n",
        "                                          ])['video_loc'].transform('max')\n",
        "    data['video_loc_std'] = data.groupby(['item_id', 'course_id'\n",
        "                                          ])['video_loc'].transform('std')\n",
        "    data['video_loc_skew'] = data.groupby(\n",
        "        ['item_id', 'course_id'])['video_loc'].transform('skew').fillna(0)\n",
        "\n",
        "    data['video_loc_mean/course_len'] = data['video_loc_mean'] / data[\n",
        "        'course_len']\n",
        "    data['video_loc_min/course_len'] = data['video_loc_min'] / data[\n",
        "        'course_len']\n",
        "    data['video_loc_max/course_len'] = data['video_loc_max'] / data[\n",
        "        'course_len']\n",
        "\n",
        "    data['video_loc_diff'] = data.groupby(\n",
        "        ['item_id',\n",
        "         'course_id'])['video_loc'].transform(lambda x: x.diff().fillna(0))\n",
        "    data['video_loc_diff_mean'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().mean()).fillna(0)\n",
        "    data['video_loc_diff_max'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().max()).fillna(0)\n",
        "    data['video_loc_diff_min'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().min()).fillna(0)\n",
        "    data['video_loc_diff_std'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().std()).fillna(0)\n",
        "    data['video_loc_diff_skew'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().skew()).fillna(0)\n",
        "\n",
        "    data['video_loc_diff/course_len'] = data['video_loc_diff'] / data[\n",
        "        'course_len']\n",
        "    data['video_loc_diff_mean/course_len'] = data[\n",
        "        'video_loc_diff_mean'] / data['course_len']\n",
        "\n",
        "    data['video_loc_ddiff'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().fillna(0))\n",
        "    data['video_loc_ddiff_mean'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().mean()).fillna(0)\n",
        "    data['video_loc_ddiff_max'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().max()).fillna(0)\n",
        "    data['video_loc_ddiff_min'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().min()).fillna(0)\n",
        "    data['video_loc_ddiff_std'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().std()).fillna(0)\n",
        "    data['video_loc_ddiff_skew'] = data.groupby([\n",
        "        'item_id', 'course_id'\n",
        "    ])['video_loc'].transform(lambda x: x.diff().diff().skew()).fillna(0)\n",
        "\n",
        "    data['video_loc_diff_lt0_count'] = data.groupby(\n",
        "        ['item_id',\n",
        "         'course_id'])['video_loc_diff'].transform(lambda x: x[x < 0].shape[0])\n",
        "    data['video_loc_diff_gt1_count'] = data.groupby(\n",
        "        ['item_id',\n",
        "         'course_id'])['video_loc_diff'].transform(lambda x: x[x > 1].shape[0])\n",
        "\n",
        "    data['video_loc_diff_lt0_count/item_coures_video_nunique'] = data['video_loc_diff_lt0_count'] / \\\n",
        "        data['item_coures_video_nunique']\n",
        "    data['video_loc_diff_gt1_count/item_coures_video_nunique'] = data['video_loc_diff_gt1_count'] / \\\n",
        "        data['item_coures_video_nunique']\n",
        "\n",
        "    #     item + course_id + time\n",
        "    print('item + course_id + time')\n",
        "    for i in [\n",
        "            'local_watching_time', 'video_progress_time',\n",
        "            'local_watching_time/video_duration'\n",
        "    ]:\n",
        "        data[f'{i}_mean'] = data.groupby(['item_id',\n",
        "                                          'course_id'])[i].transform('mean')\n",
        "        data[f'{i}_max'] = data.groupby(['item_id',\n",
        "                                         'course_id'])[i].transform('max')\n",
        "        data[f'{i}_min'] = data.groupby(['item_id',\n",
        "                                         'course_id'])[i].transform('min')\n",
        "        data[f'{i}_std'] = data.groupby(['item_id',\n",
        "                                         'course_id'])[i].transform('std')\n",
        "\n",
        "        data[f'{i}_diff'] = data.groupby(\n",
        "            ['item_id',\n",
        "             'course_id'])[i].transform(lambda x: x.diff()).fillna(0)\n",
        "    \n",
        "    print('time')\n",
        "    data['time_end_start'] = data['video_end_time'] - data['video_start_time']\n",
        "    data['time_end-start/video_duration'] = data[\n",
        "        'time_end_start'] / data['video_duration']\n",
        "\n",
        "    print('item + course_id + time')\n",
        "    data['item_course_video_finished_watching_count'] = data.groupby(['item_id', 'course_id'])[\n",
        "        'time_end-start/video_duration'].transform(lambda x: x[x >= 1].shape[0])\n",
        "    data['item_course_video_finished_watching_ratio'] = data['item_course_video_finished_watching_count'] / \\\n",
        "        data['item_coures_video_nunique']\n",
        "    \n",
        "    print('target encode stat')\n",
        "    data['course_id_target_enc_item_mean'] = data.groupby('item_id')['course_id_target_enc'].transform('mean')\n",
        "    data['course_id_target_enc_item_max'] = data.groupby('item_id')['course_id_target_enc'].transform('max')\n",
        "    \n",
        "    data['video_id_target_enc_item_mean'] = data.groupby('item_id')['video_id_target_enc'].transform('mean')\n",
        "    data['video_id_target_enc_item_max'] = data.groupby('item_id')['video_id_target_enc'].transform('max')\n",
        "    \n",
        "    data['video_id_target_enc_item_course_mean'] = data.groupby(['item_id', 'course_id'])['video_id_target_enc'].transform('mean')\n",
        "    data['video_id_target_enc_item_course_max'] = data.groupby(['item_id', 'course_id'])['video_id_target_enc'].transform('max')\n",
        "    \n",
        "   \n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T02:31:27.163425Z",
          "start_time": "2020-11-01T01:02:04.155787Z"
        },
        "id": "8q_50104x9hh",
        "outputId": "c2e18a7a-4f7a-47c2-f945-e3bea2f2c198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "course/video encode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:16,  3.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:07,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "item\n",
            "item + course\n",
            "item + course_id + time\n",
            "time\n",
            "item + course_id + time\n",
            "target encode stat\n",
            "course attn vec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 44992/44992 [06:07<00:00, 122.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "video attn vec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 44992/44992 [07:54<00:00, 94.82it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [34:44<00:00, 65.15s/it]\n"
          ]
        }
      ],
      "source": [
        "data = create_features(data)\n",
        "data = create_vec_features(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-01T03:04:51.023794Z",
          "start_time": "2020-11-01T02:31:27.569634Z"
        },
        "id": "DxHCI3Q-x9hj"
      },
      "outputs": [],
      "source": [
        "data.to_csv('./xgb_data.csv', index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuoRGiELx9hk"
      },
      "source": [
        "## course/video tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:50:27.085518Z",
          "start_time": "2020-10-30T04:50:22.059916Z"
        },
        "id": "OKUttdwRx9hk"
      },
      "outputs": [],
      "source": [
        "course_video_tfidf = data.groupby('item_id').agg({\n",
        "    'drop':'sum',\n",
        "    \n",
        "    'course_id':lambda x:' '.join(x.tolist()),\n",
        "    'video_id':lambda x:' '.join(x.tolist()),\n",
        "})\n",
        "course_video_tfidf['drop'] = course_video_tfidf['drop'] / course_video_tfidf['course_id'].apply(lambda x: len(set(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:50:40.402737Z",
          "start_time": "2020-10-30T04:50:27.087481Z"
        },
        "id": "0GfMmkwZx9hl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "course_tfidf_vector = TfidfVectorizer(min_df=30).fit(\n",
        "    course_video_tfidf['course_id'].tolist())\n",
        "video_tfidf_vector = TfidfVectorizer(min_df=30).fit(\n",
        "    course_video_tfidf['video_id'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:50:40.434617Z",
          "start_time": "2020-10-30T04:50:40.404700Z"
        },
        "code_folding": [],
        "id": "iyRFybMcx9hl"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "def create_csr_mat_input(course_list, video_list):\n",
        "    return sp.hstack((course_tfidf_vector.transform(course_list),\n",
        "                      video_tfidf_vector.transform(video_list)),\n",
        "                     format='csr')\n",
        "\n",
        "\n",
        "def kfold_tfidf_model(df_train, df_test, target, model_list):\n",
        "    folds = KFold(n_splits=5, random_state=42)\n",
        "    for model in model_list:\n",
        "        df_test[f'{model.__class__.__name__}_pred'] = 0\n",
        "\n",
        "    test_x = create_csr_mat_input(df_test['course_id'].tolist(),\n",
        "                                  df_test['video_id'].tolist())\n",
        "\n",
        "    for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(df_train))):\n",
        "        tr_x, tr_y = create_csr_mat_input(\n",
        "            df_train['course_id'].iloc[trn_idx].tolist(), df_train['video_id'].\n",
        "            iloc[trn_idx].tolist()), df_train[target].iloc[trn_idx]\n",
        "        vl_x, vl_y = create_csr_mat_input(\n",
        "            df_train['course_id'].iloc[val_idx].tolist(), df_train['video_id'].\n",
        "            iloc[val_idx].tolist()), df_train[target].iloc[val_idx]\n",
        "\n",
        "        for model in model_list:\n",
        "            if model.__class__.__name__ == 'LGBMRegressor' or model.__class__.__name__ == 'XGBRegressor':\n",
        "                model.fit(tr_x,\n",
        "                          tr_y,\n",
        "                          eval_set=[(vl_x, vl_y)],\n",
        "                          early_stopping_rounds=400,\n",
        "                         verbose=99999)\n",
        "            else:\n",
        "                model.fit(tr_x, tr_y)\n",
        "            y_pred = model.predict(vl_x)\n",
        "            print(\n",
        "                f'fold {fold_+1} model: {model.__class__.__name__} r2_score:{r2_score(vl_y, y_pred)}'\n",
        "            )\n",
        "            df_train.loc[df_train.iloc[val_idx, :].\n",
        "                         index, f'{model.__class__.__name__}_pred'] = y_pred\n",
        "\n",
        "            df_test[f'{model.__class__.__name__}_pred'] += model.predict(\n",
        "                test_x) / folds.n_splits\n",
        "    return pd.concat([df_train, df_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:50:43.105797Z",
          "start_time": "2020-10-30T04:50:40.436614Z"
        },
        "id": "F80SAQEox9hl"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor, SGDRegressor, ARDRegression, ElasticNet\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "estimators = [\n",
        "    RandomForestRegressor(n_estimators=300,\n",
        "                          max_depth=20,\n",
        "                          min_samples_split=20,\n",
        "                          random_state=42,\n",
        "                          n_jobs=-1),\n",
        "    GradientBoostingRegressor(n_estimators=350,\n",
        "                              learning_rate=0.1,\n",
        "                              random_state=42,\n",
        "                              max_features='auto',\n",
        "                              n_iter_no_change=50),\n",
        "    PassiveAggressiveRegressor(random_state=42, early_stopping=True),\n",
        "    SGDRegressor(random_state=42, early_stopping=True),\n",
        "    LGBMRegressor(n_estimators=9999,\n",
        "                  learning_rate=0.05,\n",
        "                  n_jobs=-1,\n",
        "                  random_state=42,\n",
        "                  colsample_bytree=0.8,\n",
        "                  subsample=0.8),\n",
        "    XGBRegressor(\n",
        "        n_estimators=9999,\n",
        "        learning_rate=0.05,\n",
        "        colsample_bytree=0.8,\n",
        "        max_depth=10,\n",
        "        subsample=0.8,\n",
        "        random_state=42,\n",
        "        tree_method='gpu_hist',\n",
        "        predictor='gpu_predictor',\n",
        "        gpu_id=0\n",
        "    ),\n",
        "]\n",
        "\n",
        "course_video_tfidf = kfold_tfidf_model(\n",
        "    course_video_tfidf.\n",
        "    loc[~course_video_tfidf.index.str.startswith('T', na=False), :],\n",
        "    course_video_tfidf.loc[course_video_tfidf.index.str.\n",
        "                           startswith('T', na=False), :],\n",
        "    'drop',estimators\n",
        ")\n",
        "course_video_tfidf.to_csv('./course_video_tfidf.csv' , index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:51:22.206634Z",
          "start_time": "2020-10-30T04:50:43.107748Z"
        },
        "id": "uFferZnDx9hm"
      },
      "outputs": [],
      "source": [
        "course_video_tfidf = pd.read_csv('./course_video_tfidf.csv')\n",
        "for model in estimators:\n",
        "    data[f'{model.__class__.__name__}_pred'] = data['item_id'].map(course_video_tfidf[f'{model.__class__.__name__}_pred'])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:51:22.300356Z",
          "start_time": "2020-10-30T04:51:22.242512Z"
        },
        "id": "_1YRVuv-x9hm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "# lda_course = LatentDirichletAllocation(n_jobs=-1,\n",
        "#                                       random_state=2020,\n",
        "#                                       n_components=16)\n",
        "# course_video_tfidf[[\n",
        "#     f'lda_course{i+1}' for i in range(lda_course.n_components)\n",
        "# ]] = pd.DataFrame(lda_course.fit_transform(\n",
        "#     course_tfidf_vector.transform(course_video_tfidf['course_id'].tolist())),\n",
        "#                   index=course_video_tfidf.index)\n",
        "\n",
        "# for column_name in [\n",
        "#     f'lda_course{i+1}' for i in range(lda_course.n_components)\n",
        "# ]:\n",
        "#     data[column_name] = data['item_id'].map(course_video_tfidf[column_name])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:52:37.954227Z",
          "start_time": "2020-10-30T04:51:22.305342Z"
        },
        "id": "mBx2B2iex9hm"
      },
      "outputs": [],
      "source": [
        "lda_video = LatentDirichletAllocation(n_jobs=-1,\n",
        "                                      random_state=2020,\n",
        "                                      n_components=16)\n",
        "course_video_tfidf[[\n",
        "    f'lda_video{i+1}' for i in range(lda_video.n_components)\n",
        "]] = pd.DataFrame(lda_video.fit_transform(\n",
        "    video_tfidf_vector.transform(course_video_tfidf['video_id'].tolist())),\n",
        "                  index=course_video_tfidf.index)\n",
        "\n",
        "for column_name in [f'lda_video{i+1}' for i in range(lda_video.n_components)]:\n",
        "    data[column_name] = data['item_id'].map(course_video_tfidf[column_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:53:01.876492Z",
          "start_time": "2020-10-30T04:52:37.959214Z"
        },
        "id": "0VsvjJwwx9hm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf_video = NMF(random_state=2020, n_components=16)\n",
        "course_video_tfidf[[\n",
        "    f'nmf_video{i+1}' for i in range(nmf_video.n_components)\n",
        "]] = pd.DataFrame(nmf_video.fit_transform(\n",
        "    video_tfidf_vector.transform(course_video_tfidf['video_id'].tolist())),\n",
        "                  index=course_video_tfidf.index)\n",
        "\n",
        "for column_name in [f'nmf_video{i+1}' for i in range(nmf_video.n_components)]:\n",
        "    data[column_name] = data['item_id'].map(course_video_tfidf[column_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:53:12.256786Z",
          "start_time": "2020-10-30T04:53:01.991186Z"
        },
        "id": "Qy0mDrRyx9hn"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_video = TruncatedSVD(random_state=2020, n_components=16)\n",
        "course_video_tfidf[[\n",
        "    f'svd_video{i+1}' for i in range(svd_video.n_components)\n",
        "]] = pd.DataFrame(svd_video.fit_transform(\n",
        "    video_tfidf_vector.transform(course_video_tfidf['video_id'].tolist())),\n",
        "                  index=course_video_tfidf.index)\n",
        "\n",
        "for column_name in [f'svd_video{i+1}' for i in range(svd_video.n_components)]:\n",
        "    data[column_name] = data['item_id'].map(course_video_tfidf[column_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfe-VeWBx9hn"
      },
      "source": [
        "## drop columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:53:12.272711Z",
          "start_time": "2020-10-30T04:53:12.258749Z"
        },
        "id": "E4j97JOAx9hn"
      },
      "outputs": [],
      "source": [
        "drop_columns = [\n",
        "    'drop', 'item_id', 'local_end_time', 'local_start_time', 'course_year',\n",
        "    'course_year_month', 'year', 'month', 'day', 'dayofweek', 'hour',\n",
        "#     'year_target_enc', 'month_target_enc', 'dayofweek_target_enc','course_id_target_enc',\n",
        "    'video_id', 'course_id'\n",
        "]\n",
        "\n",
        "features = [f for f in data.columns if f not in drop_columns]\n",
        "features = [\n",
        "    f for f in features if not f.__contains__('video_vec_diff')\n",
        "    and not f.__contains__('course_year')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vd-zIhVx9hn"
      },
      "source": [
        "# fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-30T04:54:16.955724Z",
          "start_time": "2020-10-30T04:53:12.274705Z"
        },
        "id": "AggaJxqvx9hn"
      },
      "outputs": [],
      "source": [
        "data = data.groupby(['item_id', 'course_id']).tail(1)\n",
        "data.to_csv('./ddddd.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.977Z"
        },
        "id": "AcbgqPrqx9ho"
      },
      "outputs": [],
      "source": [
        "data['course_id'] = course_encode.transform(data['course_id'])\n",
        "data['video_id'] = video_encode.transform(data['video_id'])\n",
        "X = data[~data['drop'].isna()]\n",
        "test = data[data['drop'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-07-10T01:01:09.900287Z",
          "start_time": "2020-07-10T01:01:05.100Z"
        },
        "id": "MX7UcYskx9ho"
      },
      "source": [
        "# run model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.980Z"
        },
        "code_folding": [],
        "id": "R45vs-1yx9ho"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "\n",
        "def LGB_CV(max_depth,\n",
        "           learning_rate,\n",
        "           min_child_weight,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           reg_alpha,\n",
        "           reg_lambda,\n",
        "           X_train=X,\n",
        "           Y_train=X['drop']):\n",
        "\n",
        "    folds = GroupKFold(n_splits=5)\n",
        "    oof = np.zeros(X_train.shape[0])\n",
        "\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(\n",
        "            folds.split(X_train, groups=X_train['item_id'].astype(str))):\n",
        "        #         print(\"fold n°{}\".format(fold_))\n",
        "        train_x, train_y = X_train[features].iloc[trn_idx], Y_train.iloc[\n",
        "            trn_idx]\n",
        "        val_x, val_y = X_train[features].iloc[val_idx], Y_train.iloc[val_idx]\n",
        "        \n",
        "        tmp_val_x = X_train.iloc[val_idx, :].sample(\n",
        "            frac=1, random_state=42).groupby('item_id').head(1)[features]\n",
        "        tmp_val_y = Y_train.loc[tmp_val_x.index]\n",
        "\n",
        "        model = XGBClassifier(n_estimators=9999,\n",
        "                              max_depth=int(max_depth),\n",
        "                              learning_rate=learning_rate,\n",
        "                              min_child_weight=int(min_child_weight),\n",
        "                              subsample=subsample,\n",
        "                              colsample_bytree=colsample_bytree,\n",
        "                              reg_alpha=reg_alpha,\n",
        "                              reg_lambda=reg_lambda,\n",
        "                              random_state=42,\n",
        "                              tree_method='gpu_hist',\n",
        "                              predictor='gpu_predictor',\n",
        "                              gpu_id=0)\n",
        "\n",
        "        model.fit(\n",
        "            train_x,\n",
        "            train_y,\n",
        "            eval_set=[(train_x, train_y), (tmp_val_x, tmp_val_y)],\n",
        "            #             sample_weight=train_x_weight,\n",
        "            #             eval_sample_weight=[train_x_weight, val_x_weight],\n",
        "            eval_metric='auc',\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=999999)\n",
        "\n",
        "        oof[val_idx] = model.predict_proba(\n",
        "            X_train[features].iloc[val_idx])[:, 1]\n",
        "\n",
        "        del model, trn_idx, val_idx\n",
        "\n",
        "    return roc_auc_score(Y_train, oof)\n",
        "\n",
        "\n",
        "LGB_BO = BayesianOptimization(\n",
        "    LGB_CV, {\n",
        "        'max_depth': (4, 20),\n",
        "        'learning_rate': (0.03, 0.08),\n",
        "        'min_child_weight': (0, 30),\n",
        "        'subsample': (0.5, 1.0),\n",
        "        'colsample_bytree': (0.5, 1.0),\n",
        "        'reg_alpha': (0, 4),\n",
        "        'reg_lambda': (0, 4),\n",
        "    })\n",
        "\n",
        "# LGB_BO.maximize(init_points=2, n_iter=5)\n",
        "# print(LGB_BO.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.982Z"
        },
        "code_folding": [
          3,
          29
        ],
        "id": "WRTsQtDBx9hp"
      },
      "outputs": [],
      "source": [
        "from scipy.misc import derivative\n",
        "\n",
        "\n",
        "def focal_loss_lgb(y_true, y_pred, alpha, gamma):\n",
        "    a, g = alpha, gamma\n",
        "\n",
        "    def fl(x, t):\n",
        "        p = 1 / (1 + np.exp(-x))\n",
        "        return -(a * t + (1 - a) *\n",
        "                 (1 - t)) * ((1 - (t * p + (1 - t) *\n",
        "                                   (1 - p)))**g) * (t * np.log(p) +\n",
        "                                                    (1 - t) * np.log(1 - p))\n",
        "\n",
        "    def partial_fl(x):\n",
        "        return fl(x, y_true)\n",
        "\n",
        "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
        "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "def focal_loss(x, y):\n",
        "    return focal_loss_lgb(x, y, 0.25, 1.)\n",
        "\n",
        "\n",
        "def lgb_f1_score(y_true, y_pred):\n",
        "    return 'f1', f1_score(y_true, np.round(y_pred)), True\n",
        "\n",
        "\n",
        "def find_best_t(X):\n",
        "    best_acc = 0\n",
        "    best_t = 0.5\n",
        "    for t in tqdm(range(490, 510)):\n",
        "        t /= 1000\n",
        "        #         cur_acc = X.groupby('item_id').parallel_apply(lambda x: accuracy_score(\n",
        "        #             x['drop'], np.where(x['pred'] >= t, 1, 0))).mean()\n",
        "        cur_acc = accuracy_score(X['drop'],\n",
        "                                 np.where(X['pred'] >= t, 1, 0),\n",
        "                                 sample_weight=1 / X['item_course_nunique'])\n",
        "        if cur_acc >= best_acc:\n",
        "            best_t = t\n",
        "            best_acc = cur_acc\n",
        "    print(f'best_acc: {best_acc}, best_t:{best_t}')\n",
        "    return best_t\n",
        "\n",
        "\n",
        "def lgb_acc(y_true, y_pred, sample_weight):\n",
        "    return 'acc', accuracy_score(y_true,\n",
        "                                 y_pred.round(),\n",
        "                                 sample_weight=sample_weight), True\n",
        "\n",
        "\n",
        "def kfold_xgb(X, y, test_x):\n",
        "    test_y = 0\n",
        "    kf = GroupKFold(n_splits=5)\n",
        "    X['pred'] = 0\n",
        "    test_x_weight = 1 / test_x['item_course_nunique']\n",
        "    for i, (train_index, val_index) in enumerate(\n",
        "            kf.split(X, y=X['drop'], groups=X['item_id'].astype(str))):\n",
        "        train_x, train_y = X.iloc[train_index, :][features], y.iloc[\n",
        "            train_index]\n",
        "        val_x, val_y = X.iloc[val_index, :][features], y.iloc[val_index]\n",
        "        tmp_val_x = X.iloc[val_index, :].sample(\n",
        "            frac=1, random_state=42).groupby('item_id').head(1)[features]\n",
        "        tmp_val_y = y.loc[tmp_val_x.index]\n",
        "\n",
        "        train_x_weight = 1 / train_x['item_course_nunique']\n",
        "        val_x_weight = 1 / val_x['item_course_nunique']\n",
        "\n",
        "        params = {\n",
        "            'colsample_bytree': 0.7010984508141815,\n",
        "            'learning_rate': 0.037811671319995785,\n",
        "            'max_depth': int(14.356534206010181),\n",
        "            'min_child_weight': 15.654280256106746,\n",
        "            'reg_alpha': 2.4870077753307727,\n",
        "            'reg_lambda': 1.123720654835033,\n",
        "            'subsample': 0.8312777128028025\n",
        "        }\n",
        "\n",
        "        model = XGBClassifier(n_estimators=9999,\n",
        "                              random_state=42,\n",
        "                              tree_method='gpu_hist',\n",
        "                              predictor='gpu_predictor',\n",
        "                              gpu_id=0,\n",
        "                              **params)\n",
        "        model.fit(\n",
        "            train_x,\n",
        "            train_y,\n",
        "            eval_set=[(train_x, train_y), (tmp_val_x, tmp_val_y)],\n",
        "            #             sample_weight=train_x_weight,\n",
        "            #             eval_sample_weight=[train_x_weight, val_x_weight],\n",
        "            eval_metric='auc',\n",
        "            early_stopping_rounds=400,\n",
        "            verbose=100)\n",
        "        print(\n",
        "            f'{i+1} fold accuracy_score is {accuracy_score(val_y, model.predict(val_x), sample_weight=val_x_weight)}'\n",
        "        )\n",
        "        #\n",
        "\n",
        "        #         retrain\n",
        "        best_iteration_ = model.best_iteration\n",
        "#         retrain_x = test_x\n",
        "        retrain_x = pd.concat([test_x, val_x])\n",
        "        #         first round\n",
        "        retrain_idx = model.predict_proba(retrain_x)[:, 1]\n",
        "        retrain_idx = (retrain_idx >= 0.95) | (retrain_idx <= 0.05)\n",
        "        copy_test_x = retrain_x.copy().iloc[retrain_idx, :]\n",
        "        copy_test_y = pd.Series(model.predict(copy_test_x))\n",
        "        model.set_params(n_estimators=best_iteration_ + 10, verbose=300)\n",
        "        model.fit(\n",
        "            pd.concat([train_x, copy_test_x]),\n",
        "            pd.concat([train_y, copy_test_y]),\n",
        "            #                   sample_weight=pd.concat([train_x_weight, test_x_weight])\n",
        "        )\n",
        "        #         second round\n",
        "        retrain_idx = model.predict_proba(retrain_x)[:, 1]\n",
        "        retrain_idx = (retrain_idx >= 0.95) | (retrain_idx <= 0.05)\n",
        "        copy_test_x = retrain_x.copy().iloc[retrain_idx, :]\n",
        "        copy_test_y = pd.Series(model.predict(copy_test_x))\n",
        "        model.set_params(n_estimators=best_iteration_ + 10, verbose=300)\n",
        "        model.fit(\n",
        "            pd.concat([train_x, copy_test_x]),\n",
        "            pd.concat([train_y, copy_test_y]),\n",
        "            #                   sample_weight=pd.concat([train_x_weight, test_x_weight])\n",
        "        )    \n",
        "        \n",
        "        print(\n",
        "            f'{i+1} fold retrain accuracy_score is {accuracy_score(val_y, model.predict(val_x), sample_weight=val_x_weight)}'\n",
        "        )\n",
        "\n",
        "        X.loc[val_x.index, 'pred'] = model.predict_proba(val_x)[:, 1]\n",
        "        test_y += model.predict_proba(test_x) / kf.n_splits\n",
        "\n",
        "    total_acc = X.groupby('item_id').apply(\n",
        "        lambda x: accuracy_score(x['drop'], x['pred'].round())).mean()\n",
        "    print(f'total accuracy_score is {total_acc}')\n",
        "\n",
        "    #         post oper\n",
        "    #     t = find_best_t(X)\n",
        "    t = 0.5\n",
        "\n",
        "    xgb.plot_importance(model, max_num_features=30, height=0.5)\n",
        "    plt.show()\n",
        "    return np.where(test_y[:, 1] >= t, 1, 0), test_y[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.984Z"
        },
        "scrolled": true,
        "id": "exbG8lPjx9hp"
      },
      "outputs": [],
      "source": [
        "X.fillna(0, inplace=True)\n",
        "y_pred, y_pred_prob = kfold_xgb(X, X['drop'], test[features].fillna(0))\n",
        "# total accuracy_score is 0.8657915776254426 online:0.867983668515019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhjB1h__x9hp"
      },
      "source": [
        "# submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.986Z"
        },
        "id": "81TtRACBx9hq"
      },
      "outputs": [],
      "source": [
        "# test['drop'] = y_pred.argmax(axis=1)\n",
        "test['drop'] = y_pred\n",
        "test['drop_prob'] = y_pred_prob\n",
        "test['course_id'] = course_encode.inverse_transform(test['course_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.988Z"
        },
        "id": "quOlefp3x9hq"
      },
      "outputs": [],
      "source": [
        "test[['item_id', 'course_id', 'drop_prob']].to_csv('./test_drop_prob_xgb.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.991Z"
        },
        "id": "_ZHcc2GOx9hq"
      },
      "outputs": [],
      "source": [
        "noLabel['label_list'] = noLabel.apply(lambda x: [\n",
        "    test.loc[(test['item_id'] == x['item_id']) &\n",
        "             (test['course_id'] == c), 'drop'].iloc[0]\n",
        "    for c in x['course_list']\n",
        "],\n",
        "    axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-28T15:45:06.993Z"
        },
        "id": "qdo27mJex9hq"
      },
      "outputs": [],
      "source": [
        "with open('./submit/submit_xgb.json',\"w+\") as f:    #设置文件对象\n",
        "    f.write(noLabel[['label_list', 'item_id']].to_json(orient='records')[1:-1].replace(',{', '\\n{'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwQRdnzYx9hq"
      },
      "outputs": [],
      "source": [
        "X[['item_id', 'course_id', 'pred']].to_csv('./train_pred.csv', index=None)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "296px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "学生退课行为预测.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}